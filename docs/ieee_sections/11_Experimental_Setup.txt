Experimental Setupâ€”Vectron includes a benchmarking and testing framework that starts a full system stack, executes defined workloads, and collects detailed metrics. The benchmark suite defines dataset sizes ranging from small (1K vectors) to large (10K vectors) and supports multiple vector dimensionalities (128, 384, 768, 1536). It creates temporary directories for data and logs, ensuring isolation between runs. Services are started with explicit port mappings to avoid conflicts with other test suites.

The framework measures ingestion throughput by inserting vectors in batches and computing vectors per second. Search throughput is measured by issuing concurrent search queries and computing queries per second. Latency is recorded for each operation, and the system computes P50, P95, P99, mean, and standard deviation. Search quality is evaluated by computing recall and precision at multiple K values, F1 score, mean reciprocal rank, and NDCG. The benchmark suite also records memory usage and goroutine counts to provide insight into resource usage under load.

End-to-end tests complement benchmarks by validating system behavior under realistic scenarios. The tests start all services, register users, obtain JWTs, create collections, upsert vectors, perform searches, and validate results. They include reranking tests to verify improved ranking behavior and persistence tests to ensure data survives restarts. They also include failure scenarios, such as stale routing errors and partial worker unavailability.

The experimental setup assumes a local development environment with a multi-process deployment of PD, workers, Gateway, Auth, and Reranker. The use of reproducible temp directories and explicit timeouts ensures consistent evaluation. Although the codebase does not ship fixed results tables, the benchmark suite is designed to produce such results in a reproducible manner.
