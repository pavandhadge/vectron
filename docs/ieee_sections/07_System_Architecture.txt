System Architecture—Vectron is organized as a multi‑service system that separates control and data responsibilities while maintaining a coherent public interface. The architecture comprises five primary services and a management frontend: the API Gateway, the Placement Driver, the Worker nodes, the Auth Service, the Reranker service, and a React-based management console. Each component is designed with a specific operational boundary, and their interactions are explicitly controlled to preserve correctness, durability, and scalability.

The API Gateway is the sole public entry point for all client operations. It exposes gRPC endpoints and HTTP/JSON endpoints generated via grpc‑gateway. Clients authenticate through JWTs and API keys, and the gateway validates tokens, enforces rate limits, and applies logging policies. The gateway’s role is not limited to access control; it is the system’s orchestration layer. It is responsible for routing requests to the correct shard, aggregating responses across distributed workers, caching results to reduce repeated work, and optionally invoking the Reranker for post-search refinement. The gateway also exposes management endpoints that return system health, worker lists, collection metadata, and operational statistics. This centralization allows internal data‑plane protocols to evolve while keeping the public API stable.

The Placement Driver (PD) is the control‑plane service that maintains authoritative cluster metadata. It is itself a Raft cluster, ensuring consistent state replication across PD nodes. PD stores worker registrations, collection definitions, shard key ranges, replica assignments, leader identities, and shard epochs. It also tracks worker liveness via heartbeats and uses this information to assign or reassign shards. PD’s design reflects its role as a stable, low‑throughput control service: it is intended to remain responsive even when the data plane experiences high load. PD performs reconciliation through a background loop that repairs under‑replicated shards, removes dead workers, and triggers leader recovery for shards with missing leaders. This loop embodies the control plane’s role as the system’s stabilizer and repair agent.

Workers implement the data plane. Each Worker hosts multiple shard replicas, and each shard is an independent Raft group. The Worker uses Dragonboat’s NodeHost to manage these Raft groups. A key architectural choice is that the shard boundary is also the storage and indexing boundary: each shard has its own state machine, PebbleDB database, and HNSW index. This mapping simplifies correctness and recovery; a shard is an atomic unit of replication, durability, and search. Workers expose a gRPC API that includes vector CRUD operations, search APIs, and index replication streams. Workers do not expose public endpoints; they only serve the Gateway and PD.

The Auth Service is an independent identity and credential manager. It stores user accounts, API keys, and subscription plans in etcd. It issues JWTs used for Gateway authentication and provides RPCs for validating SDK tokens. The Auth Service has both gRPC and HTTP endpoints, serving SDKs and the management frontend. Separating Auth from the Gateway allows authentication logic to evolve independently and keeps the Gateway focused on orchestration rather than account management.

The Reranker service provides optional post‑search ranking refinement. It is intentionally decoupled from the Gateway and Workers. The Gateway sends candidate results to the Reranker when reranking is enabled, and the Reranker returns re‑scored and reordered results. The Reranker’s strategy interface supports multiple ranking algorithms, although the current implementation is rule‑based. This modular design allows future integration of learned models without modifying the data plane.

The management frontend is a React application that consumes the Auth and Gateway HTTP endpoints. It provides user-facing features such as account management and API key creation, and system-facing views such as worker lists, collection status, and system health. The frontend is not on the data path, but it is a critical operational interface for monitoring and management.

The system’s data and control flows can be described in detail. When a client issues an Upsert, the Gateway validates the request, resolves shard routing via PD metadata, and batches points per shard and worker. It then sends batch or streaming RPCs to Workers. Workers propose the writes to their shard Raft groups, apply them to the state machine, persist them in PebbleDB, and update the HNSW index. When a client issues a Search, the Gateway checks caches, resolves routing, sends search requests to Workers, aggregates results, optionally reranks them, and returns the response. The Gateway may fan out to all workers or target specific shards depending on configuration. These flows demonstrate that the Gateway is the orchestration layer, PD provides routing metadata, and Workers execute the storage and search operations.

Routing metadata flows from PD to Gateway and Workers. Workers send heartbeats to PD, which updates its metadata and computes shard assignments. PD responses include shard assignments that tell workers which shards to host, whether to bootstrap or join, and what the initial membership should be. The Gateway queries PD for collection routing, caches the metadata, and uses it to route requests. This metadata flow is critical to system correctness, and PD’s Raft replication ensures that metadata changes are consistent across PD replicas.

Failure handling is embedded in the architecture. If a worker fails, PD detects missing heartbeats and triggers reconciliation to repair under‑replicated shards. If the Gateway encounters stale shard metadata, workers return epoch mismatch errors, prompting the Gateway to refresh routing and retry. If PD leadership changes, the Gateway and Workers update their leader connection by probing PD nodes. The system is thus designed to tolerate failures without requiring manual intervention in most cases.

The system architecture is also influenced by scaling considerations. The Gateway can be scaled horizontally behind a load balancer, provided it shares distributed caches or uses consistent hashing for routing. Workers scale by adding nodes and redistributing shards. PD remains small and stable, with its Raft cluster providing high availability for metadata operations. The Reranker can be scaled independently if reranking becomes a hotspot. The system’s configuration model supports these scaling strategies by allowing each service to manage its own dependency addresses and performance parameters.

A textual diagram can be described as follows. At the top, clients connect to the API Gateway. The Gateway connects to PD for routing information. The Gateway connects to Workers for data operations. The Gateway optionally connects to the Reranker for post‑search processing. The Gateway also connects to the Auth Service for token validation. Workers connect to PD for registration and heartbeats. The frontend connects to the Auth Service for user operations and to the Gateway for management operations. This diagram emphasizes that the Gateway is the only public-facing data-plane interface, PD is the only authoritative control-plane metadata store, and Workers are the only nodes that store vector data and execute search.

The architectural decisions can be tied directly to code modules. The Gateway’s orchestration logic is implemented in `apigateway/cmd/apigateway/main.go`, with configuration in `apigateway/cmd/apigateway/config.go`. The Placement Driver’s core logic is implemented in `placementdriver/internal/server` and `placementdriver/internal/fsm`. Worker shard management and Raft integration live in `worker/internal/shard` and `worker/internal/raft`. The HNSW index and storage engine reside in `worker/internal/idxhnsw` and `worker/internal/storage`. The Auth Service is implemented in `auth/service`, and the Reranker in `reranker/internal`. The frontend resides in `auth/frontend`. These module boundaries reinforce the architectural separation described above.

The architecture embodies a specific research position. Rather than treating vector search as an indexing problem alone, Vectron treats it as a distributed systems problem. It integrates durable storage, consensus, and approximate search into a cohesive design. This integration is not trivial: it requires aligning shard boundaries with index boundaries, ensuring that replication and index state remain consistent, and providing a coherent orchestration layer that can manage routing and caching. The system architecture is therefore the principal contribution, and its design choices are motivated by production requirements such as durability, scalability, and operational clarity.

In summary, Vectron’s system architecture is defined by a clear separation of roles. The Gateway orchestrates, the Placement Driver coordinates, Workers store and search, Auth secures, and the Reranker refines. The system provides explicit control over trade-offs and ensures that core flows are deterministic and recoverable. This architecture provides the foundation upon which the system’s algorithms and operational workflows are built.
