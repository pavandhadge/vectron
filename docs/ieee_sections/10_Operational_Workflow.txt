Operational Workflow—Vectron’s operational lifecycle starts with service initialization. Each service loads its configuration from its environment file. The Placement Driver starts its Raft node and gRPC server. Workers start their NodeHost, initialize storage and indexing, and then register with the Placement Driver. The Gateway starts its gRPC and HTTP servers, initializes caches and distributed cache clients, and connects to the Placement Driver and Auth Service. The Auth Service connects to etcd and starts its gRPC and HTTP servers. The Reranker starts with its configured strategy and cache backend. The frontend uses its configured base URLs to connect to the Auth Service and Gateway.

Ingestion begins with a client Upsert request. The Gateway validates the collection name and point data, then resolves shard placement. It groups points by shard and worker, and dispatches batched write requests. For large batches, it uses streaming RPCs to reduce overhead and support backpressure. Workers receive the batch, translate it into commands, and propose them to the shard Raft log. Once committed, the state machine applies the commands: the vectors are stored in PebbleDB and the HNSW index is updated synchronously or asynchronously. The worker returns the result to the Gateway, which aggregates per-batch success and returns a response to the client.

Search begins with a Search request. The Gateway validates the request, sets default topK if missing, and checks its caches. If rerank warmup caching is enabled, it may return a warmup result. It then checks the search cache, which may be local or distributed. If a cache miss occurs, the Gateway resolves routing. It may fan out to all workers or target a subset depending on configuration. Workers perform HNSW searches and return candidate IDs and scores. The Gateway aggregates results using a heap, truncates to topK, optionally reranks, caches the final response, and returns it. SearchStream follows the same logic but streams partial results as they arrive.

Failure handling is integrated into each stage. If a worker is unreachable, the Gateway can return a partial response if configured, otherwise it fails the request. If a stale shard epoch is detected, the Gateway refreshes routing metadata and retries. If the Placement Driver leader changes, the Gateway and workers update their leader connection. Worker failures are detected by PD through heartbeat timeouts, and the reconciliation loop adds replicas or reassigns shards. Recovery of shard state uses Raft snapshots, Pebble recovery, and HNSW WAL replay. This workflow ensures eventual convergence and service continuity under failures.

Operational observability is built into the system. Hot-path logging can be sampled to avoid overhead. Profiling hooks can be enabled for CPU and lock contention analysis. The Gateway exposes management endpoints that report worker status, collection health, and system metrics. The frontend presents these endpoints to operators.
