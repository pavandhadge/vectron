Results and Evaluation—Vectron’s evaluation emphasizes end-to-end system behavior and search quality rather than isolated microbenchmarks. The benchmark suite measures latency distributions for both ingestion and search, capturing median and tail latencies. It measures throughput in terms of vectors ingested per second and queries served per second. Search quality is measured using recall, precision, F1, MRR, and NDCG. These metrics enable the evaluation of trade-offs between search accuracy and latency as HNSW parameters are tuned.

The results derived from the benchmark framework show that approximate search yields low latency at scale and that increasing efSearch improves recall at the cost of higher latency. Two-stage search provides a middle ground by limiting candidate generation cost while refining results. Reranking improves relevance metrics when metadata signals align with query intent but introduces additional latency, which can be bounded through configurable timeouts. Caching significantly reduces repeated query latency, particularly in workloads with skewed query distributions.

Write throughput benefits from batching and asynchronous indexing. When asynchronous indexing is enabled, ingestion throughput increases because index updates are decoupled from write commits. The system includes a configurable search wait window that can mitigate empty result sets when indexing lags behind. This allows operators to trade off immediate search freshness for ingestion throughput.

The evaluation framework also captures resource usage. Memory usage is influenced by HNSW parameters, quantization settings, and hot index configuration. The system provides knobs to balance memory footprint and search accuracy. The worker’s use of object pooling and cache sharding reduces GC pressure and improves throughput under load. The gateway’s concurrency limits prevent oversubscription under high fanout.

While the codebase does not provide a fixed set of published results, the evaluation framework provides the structure to generate such results across varying configurations. The design of the benchmarks supports reproducible experiments and can be extended to additional workloads.
