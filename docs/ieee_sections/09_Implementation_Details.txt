Implementation Details—Vectron is implemented as a set of Go microservices with a shared protobuf contract and a dedicated frontend. The core runtime services—the API Gateway, Placement Driver, Workers, Auth Service, and Reranker—are written in Go and communicate via gRPC. The public API is defined in Protocol Buffers and compiled into Go, Python, and JavaScript client libraries. The Gateway exposes HTTP/JSON endpoints via grpc‑gateway, enabling REST-style integration for environments where gRPC is not preferred. This section describes the implementation in detail, focusing on module responsibilities, concurrency mechanisms, and performance considerations as realized in the codebase.

The API Gateway is implemented in `apigateway/cmd/apigateway/main.go` and configured by `apigateway/cmd/apigateway/config.go`. The configuration model is environment-driven, with defaults for network addresses, caching policies, and reranking behavior. Configuration parsing includes boolean, integer, and structured key-value maps to support per-collection overrides. This design allows runtime behavior to be modified without recompilation and aligns with the system’s philosophy of explicit trade-offs. The Gateway constructs gRPC server options for message size limits, keepalive, and compression. It also builds gRPC client options for upstream services, ensuring consistent buffer sizes and optional gzip compression. These options are crucial in large-vector workloads where message payloads can be sizable.

The Gateway’s initialization phase constructs a set of caches and shared components. The search cache is implemented as a sharded TinyLFU structure with TTL; sharding reduces lock contention. The cache is optional and can be backed by a distributed Redis/Valkey layer for cross-instance reuse. Routing caches are maintained for collection metadata and worker lists, and a worker role cache stores role information for search-only workers. The Gateway also maintains an in-flight request map to deduplicate concurrent identical searches, reducing redundant work under bursty loads. Each of these components is initialized based on configuration, with defaults that scale with CPU or memory size to balance performance and resource usage.

The Gateway’s request path is structured around gRPC handlers with middleware interceptors. Authentication is implemented in `apigateway/internal/middleware/auth.go`, where JWTs are parsed and validated. For SDK JWTs, the Gateway calls the Auth Service’s internal RPC to fetch user context, injecting user ID and plan into the request context. Rate limiting is implemented in `apigateway/internal/middleware/ratelimit.go` using sharded in-memory maps, which reduces contention under high concurrency. Logging interceptors provide hot-path log sampling controlled by configuration to avoid excessive overhead. These middleware components are composed in a deterministic order to ensure authentication occurs before rate limiting and logging.

The Gateway’s routing logic is implemented in its request handlers, particularly in the search and upsert paths. The upsert handler performs validation, optionally warms routing caches, resolves shard assignments in parallel, batches points per shard, and dispatches batch RPCs to workers. Concurrency for routing resolution and dispatch is bounded by configurable semaphores, preventing oversubscription under heavy load. Large batches are handled via streaming RPCs, where the Gateway sends chunks of points and then receives a single aggregated response. Error handling includes stale shard detection: if a worker indicates an epoch mismatch, the Gateway refreshes routing metadata and retries. These behaviors are encoded as explicit control flow, avoiding hidden retries or implicit routing changes.

Search handling is more complex due to caching and optional reranking. The Gateway first checks a rerank warmup cache if enabled, then checks the search cache (local and distributed). For cache misses, the Gateway executes an uncached search flow that resolves routing metadata, determines whether to fan out to multiple workers, and dispatches search requests. Results are aggregated using a heap structure to compute the global topK. Candidate limits are applied to cap work before reranking. The rerank path constructs candidate objects from search results, optionally collects metadata, and invokes the Reranker service with a configurable timeout. If the reranker fails, the Gateway falls back to the original results. The final results are cached and returned. This implementation provides explicit points at which behavior can be tuned: fanout, cache sizes, candidate limits, rerank enablement, and timeouts.

The Placement Driver is implemented in `placementdriver/cmd/placementdriver/main.go` and the control-plane logic in `placementdriver/internal/server`. The Placement Driver uses a Raft node implementation to ensure metadata consistency. The FSM, defined in `placementdriver/internal/fsm`, stores workers, collections, and shards. Commands are encoded as JSON payloads, which provides human-readable debugging and flexible schema evolution. The PD gRPC server implements worker registration and heartbeats, and the reconciliation loop is implemented in `placementdriver/internal/server/reconciliation.go`. This loop periodically checks health reports, repairs under‑replicated shards, removes dead workers, and checks for leaderless shards. These operations are performed through Raft proposals to ensure that repairs are consistent across PD replicas.

The Worker is implemented in `worker/cmd/worker/main.go` with internal logic in `worker/internal/*`. The Worker starts by configuring runtime parameters, including gRPC limits and optional search-only mode. It initializes a Dragonboat NodeHost that will host multiple Raft clusters, one per shard. A PD client in `worker/internal/pd` discovers the PD leader by probing PD nodes and registers the worker. The Worker then starts a heartbeat loop that sends resource metrics and shard state to PD and receives shard assignments. This connection is resilient to PD leader changes and uses keepalive settings to maintain connectivity.

The shard management logic is implemented in `worker/internal/shard/manager.go`. The ShardManager maintains a map of running replicas and reconciles desired assignments from PD against the current state. It decides whether to bootstrap a shard or join an existing cluster based on assignment metadata and the presence of local data. It constructs Raft configurations with specific parameters (election RTT, heartbeat RTT, snapshot settings) to balance responsiveness with stability. Each shard is started with a state machine factory that creates an on-disk state machine instance. This factory records state machines in a map for later access, enabling other subsystems to query or stream data.

State machines are implemented in `worker/internal/shard/state_machine.go`. They encode commands in a versioned binary format, with fallback support for legacy JSON and Gob. The Update method applies StoreVector, StoreVectorBatch, and DeleteVector commands by delegating to the storage engine. The Lookup method handles SearchQuery and GetVectorQuery, returning IDs and scores or vectors and metadata. Snapshotting is implemented by copying the PebbleDB directory into a zip archive, and recovery extracts this archive to restore the database. This implementation provides deterministic state evolution and durable recovery under Raft semantics.

The storage engine is implemented in `worker/internal/storage`. It wraps PebbleDB and integrates HNSW. Initialization opens Pebble with options derived from environment variables and starts background sync loops. The storage layer supports ingest mode, which skips index updates during bulk ingestion and triggers index rebuilds on startup if a dirty marker is present. The write path uses Pebble batch operations to atomically store vector data and optional WAL entries for the index. After commit, the HNSW index is updated synchronously or asynchronously depending on configuration. Asynchronous indexing uses a bounded channel and background worker, tracking pending operations via an atomic counter. The search path uses the HNSW index and optionally waits briefly for pending indexing operations to complete when search results are empty, improving freshness under async indexing.

The HNSW implementation resides in `worker/internal/idxhnsw`. The index maintains a layered graph with neighbor lists per node. It supports vector normalization for cosine distance and optional quantization to int8. The index includes configurable pruning and warmup operations. It uses sync.Pool for vector buffers and int8 buffers to reduce allocation overhead. Search functions include standard search, search with explicit ef, and two-stage search. Internal mappings from external IDs to internal numeric IDs enable compact storage and efficient traversal. The index also supports hot index tiers for recent vectors and optional maintenance loops.

The Worker gRPC server is implemented in `worker/internal/grpc.go`. It provides Search, BatchSearch, StoreVector, BatchStoreVector, StreamBatchStoreVector, GetVector, DeleteVector, and replication stream endpoints. Writes are proposed to Raft using SyncPropose with retries for transient errors, including configurable backoff and retry counts. Reads can be linearizable or eventual depending on request flags. Broadcast search across shards is supported by issuing per-shard searches concurrently and aggregating results. The implementation uses heap structures and object pools to reduce allocation overhead. Worker-level search caches are implemented with sharded maps and TTL; cache keys can be quantized to improve hit rates for similar queries. This design reduces repeated work for common queries.

The Reranker is implemented in `reranker/cmd/reranker/main.go` and `reranker/internal`. It exposes a gRPC server that validates rerank requests, constructs candidate lists, and uses a strategy interface to compute rankings. The rule-based strategy uses metadata weights and query matching rules to adjust scores. The cache layer can be in-memory or Redis-based, and the service includes an API to invalidate cached results. The Reranker uses a lightweight logging interface and a metrics interface to record cache hits and latency, enabling integration with monitoring systems.

The Auth Service is implemented in `auth/service/cmd/auth/main.go` and `auth/service/internal`. It uses etcd as a persistence layer and bcrypt for password hashing. It validates email and password strength, normalizes emails, and stores user profiles. JWTs are issued with user ID and plan, with expiration metadata. API keys are stored hashed and only returned in full at creation time. The Auth Service exposes both gRPC and HTTP endpoints, and uses middleware to enforce JWT validation for protected operations. This implementation provides a standard identity and credential model integrated into the Gateway’s authentication flow.

The frontend is implemented in `auth/frontend`. It uses React with routing, an AuthContext for state management, and Axios clients configured with JWT headers. The frontend interacts with Auth and Gateway HTTP endpoints for user operations and management views. It includes pages for login, signup, profile, API keys, collections, vectors, and management dashboards. The frontend is not on the data path for vector search but provides operational visibility and user-facing controls.

The client SDKs are implemented in `clientlibs` for Go, Python, and JavaScript. They are generated from protobuf definitions and wrap the Gateway API. The Go client uses gRPC with configurable TLS, keepalive, retries, and optional hedged reads. The Python and JavaScript clients provide similar options, including timeouts, compression, and retry policies. Each SDK maps gRPC error codes to typed exceptions, providing a consistent error model for clients. These SDKs are integral to the system’s usability and are maintained in the codebase with examples and documentation.

Finally, the repository includes operational scripts such as `run-all.sh` and `generate-all.sh`. The run-all script starts all services for local development, while generate-all regenerates protobuf outputs for all languages. Profiling scripts and benchmarking harnesses are included to support performance evaluation. This tooling is not ancillary but an essential part of the system’s implementation, providing reproducibility and operational clarity.

In sum, Vectron’s implementation is a tightly integrated system that reflects its architectural principles. The codebase makes explicit choices about concurrency, caching, durability, and search quality, and it provides the infrastructure to observe and tune those choices in practice.
